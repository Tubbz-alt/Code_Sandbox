\documentclass[12pt]{report}


\usepackage{fullpage}
\usepackage{algpseudocode}
\usepackage{algorithmicx}
\usepackage{amsmath}
\usepackage{amsfonts}

\begin{document}

\section*{Data Description}

The dataset contains 435 lines.  One per member of the House of Representatives.  Each 
line contains the political party, followed by their votes on 17 selected issues.  

Each vote can be \emph{yes}, \emph{no}, or \emph{no-vote}.  


\section*{Algorithm}

Because I want to consider non-voting choices, I use a Normal Distribution with the following values
for each choice.

\begin{itemize}
\item[] yes = 1
\item[] no  = 0
\item[] no-vote = 0.5
\end{itemize}

The MLE equation for computing the mean $\mu$ given a sample is therefore

\begin{equation}
\mu = \sum_{i=1}^{N} \frac{\bar{x}_i}{N}
\end{equation}

The MLE equation for standard deviation, $\sigma$, is 

\begin{equation}
\sigma = \sqrt{\frac{1}{N} \sum_{i-1}^{N} \left( x_i - \mu \right)}
\end{equation}


First, a subset of the voting entries are taken and then split into a Republican group and 
Democrat group. 

A mean and standard deviation are computed for each party, for each vote.  The end result is

\begin{equation}
\begin{matrix}
V_{r} = \{ \left( \mu_{r_1}, \sigma_{r_1} \right), \left( \mu_{r_2}, \sigma_{r_2} \right), \cdots, \left( \mu_{r_N}, \sigma_{r_N} \right) \}\\
V_{d} = \{ \left( \mu_{d_1}, \sigma_{d_1} \right), \left( \mu_{d_2}, \sigma_{d_2} \right), \cdots, \left( \mu_{d_N}, \sigma_{d_N} \right) \}
\end{matrix}
\end{equation}


\clearpage
\subsection*{Classification}

For classification, a test set with the same dimensionality as the training set is used.

A test sample is compared against each party.  For the quality metric, we use the z-score.


\begin{algorithmic}
\State
\State Given $S = \left( x_1, x_2, \cdots, x_N \right)$ where $N = $ number of votes and $x = $ (Y,N, no-vote).
\State
\State \# Set initial total to zero
\State $total\_score = 0$
\State
\State \# Iterate over each vote
\For{ $i = [1 : N]$}
    
    \State
    \State \# Compute the Republican Z-Score
    \State $Z_{r_i} = \frac{\left|\mu_{r_i} - x_i\right|}{\sigma}$
    
    \State
    \State \# Compute the Democrat Z-Score
    \State $Z_{d_i} = \frac{\left|\mu_{d_i} - x_i\right|}{\sigma}$
    \State

    \State \# Compute the Best Choice
    \If{ $Z_{r_i} < Z_{d_i}$ }
        \State $total\_score += Z_{r_i}$
    \Else
        \State $total\_score -= Z_{d_i}$
    \EndIf


\EndFor
\State
\State \# Compute Total Party
\If{ $total\_score > 0$ }
    \State $S$ is a Republican
\Else
    \State $S$ is a Democrat
\EndIf

\end{algorithmic}


%-------------------------------------%
%-       Binomial Distribution       -%
%-------------------------------------%
\clearpage
\subsection*{Binomial Distribution}

%-     Start the PMF Equations     -%
Probability Mass Function


\begin{equation}
f(n,k | p) = \begin{pmatrix}
n\\k
\end{pmatrix} p^k (1-p)^{n-k} =\left( \frac{n!}{k!(n-k)!}\right) p^k (1-p)^{n-k}
\end{equation}


%-       Likelihood Equations     -%
Likelihood

\begin{equation}
\mathfrak{L}(p|n,k) = \begin{pmatrix}
n\\k
\end{pmatrix} p^k (1-p)^{n-k} =\left( \frac{n!}{k!(n-k)!}\right) p^k (1-p)^{n-k}
\end{equation}

%-       Log Likelihood       -%
Log Likelihood

\begin{equation}
\ln \mathfrak{L}(p | n, k) = \ln \left( \frac{n!}{k!(n-k)!}\right) + k \ln p  + (n-k) \ln (1-p)
\end{equation}


%-       Log Likelihood Derivative      -%
Derivative of Log-Likelihood

\begin{equation}
\frac{\partial \ln \mathfrak{L}(p | n, k)}{\partial p} = \frac{k}{p} - \frac{(n-k)}{1 - p} = \frac{k - np}{p (1-p)}
\end{equation}


Solution 
\begin{equation}
\frac{\partial \ln \mathfrak{L}(p | n, k)}{\partial p} = 0
\end{equation}

\begin{equation}
\frac{k}{p} - \frac{(n-k)}{(1-p)} = 0
\end{equation}

\begin{equation}
\frac{k - np}{p (1-p)} = 0
\end{equation}

\begin{equation}
p = \frac{k}{n}
\end{equation}


%-------------------------------------------%
%-           Normal Distribution           -%
%-------------------------------------------%
\clearpage
\subsection*{Normal Distribution}


%-       Start the Likelihood Function       -%
Probability Density Function

\begin{equation}
f(x) = \frac{1}{\sigma \sqrt{2 \pi}} \exp^{-\frac{(x-\mu)^2}{2\sigma^2}}
\end{equation}


Likelihood Function

\begin{equation}
\mathfrak{L}(\bar{x}|\theta) = \mathfrak{L}(\bar{x}|\mu, \sigma^2) = \prod_{i=1}^n f(x_i, \mu, \sigma) 
\end{equation}

\begin{equation}
\mathfrak{L}(\bar{x}|\theta) = \prod_{i=1}^{n} \frac{1}{\sigma\sqrt{2\pi}} \exp^{-\frac{(x_i - \mu)^2}{2\sigma^2}}
\end{equation}

\begin{equation}
\mathfrak{L}(\bar{x}|\theta) = \left(\frac{1}{2\pi\sigma^2}\right)^{\frac{n}{2}} \exp^{\left( -\frac{\sum_{i=1}^n (x_i - \mu)^2}{2\sigma^2}\right)}
\end{equation}
Log Likelihood

\begin{equation}
\ln \mathfrak{L}(\bar{x}|\theta) = \ln \left(\frac{1}{2\pi\sigma^2}\right)^{\frac{n}{2}} \exp^{\left( -\frac{\sum_{i=1}^n (x_i - \mu)^2}{2\sigma^2}\right)}
\end{equation}

\begin{equation}
\ln \mathfrak{L}(\bar{x}|\theta) = \frac{n}{2} \ln \frac{1}{2\pi\sigma^2} - \frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2
\end{equation}


Derivative of Log Likelihood w.r. $\mu$

\begin{equation}
\frac{\partial \theta}{\partial \mu} \ln \mathfrak{L}(\bar{x}|\theta) = \frac{\partial \theta}{\partial \mu} \left( \frac{n}{2} \ln \frac{1}{2\pi\sigma^2}\right) - \frac{\partial \theta}{\partial \mu} \left(\frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2\right)
\end{equation}


\begin{equation}
\frac{\partial \theta}{\partial \mu} \ln \mathfrak{L}(\bar{x}|\theta) = 0 - \frac{-n \left(\bar{x} - \mu\right)}{\sigma^2}
\end{equation}

\begin{equation}
\frac{\partial \theta}{\partial \mu} \ln \mathfrak{L}(\bar{x}|\theta) = \frac{n \left(\bar{x} - \mu\right)}{\sigma^2}
\end{equation}

Maximizing

\begin{equation}
\frac{\partial \theta}{\partial \mu} \ln \mathfrak{L}(\bar{x}|\theta) = 0 
\end{equation}


\begin{equation}
\frac{n \left(\bar{x} - \mu\right)}{\sigma^2} = 0
\end{equation}

\begin{equation}
n \bar{x} - n \mu = 0
\end{equation}

\begin{equation}
\mu = \frac{\bar{x}}{n} = \sum_{i=1}^{n} \frac{x_i}{n}
\end{equation}

Derivative of Log Likelihood w.r. $\sigma$

\begin{equation}
\frac{\partial \theta}{\partial \sigma} \ln \mathfrak{L}(\bar{x}|\theta) = \frac{\partial \theta}{\partial \sigma} \left( \frac{n}{2} \ln \frac{1}{2\pi\sigma^2}\right) - \frac{\partial \theta}{\partial \mu} \left(\frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2\right)
\end{equation}

\begin{equation}
\frac{\partial \theta}{\partial \sigma} \ln \mathfrak{L}(\bar{x}|\theta) = \left(\frac{n}{2}\right) \left(-\frac{1}{\pi \sigma^{3}}\right) \left(\frac{2\pi\sigma^2}{1}\right) - \frac{1}{\sigma^3} \left( \sum_{i=1}^n (x_i - \mu)^2 \right) 
\end{equation}

\begin{equation}
\frac{\partial \theta}{\partial \sigma} \ln \mathfrak{L}(\bar{x}|\theta) = -\frac{n}{\sigma} - \frac{ \sum_{i=1}^n (x_i - \mu)^2}{\sigma^3}
\end{equation}

Maximizing

\begin{equation}
\frac{\partial \theta}{\partial \sigma} \ln \mathfrak{L}(\bar{x}|\theta) = 0
\end{equation}

\begin{equation}
-\frac{n}{\sigma} - \frac{ \sum_{i=1}^n (x_i - \mu)^2}{\sigma^3} = 0
\end{equation}

\begin{equation}
-\frac{n}{\sigma} = \frac{ \sum_{i=1}^n (x_i - \mu)^2}{\sigma^3}
\end{equation}

\begin{equation}
\sigma^2 = \frac{1}{n} \sum_{i=1}^n (x_i - \mu)^2
\end{equation}


\begin{equation}
\sigma = \sqrt{\frac{1}{n} \sum_{i=1}^n (x_i - \mu)^2}
\end{equation}

%--------------------------------------------%
%-           Uniform Distribution           -%
%--------------------------------------------%


\end{document}

